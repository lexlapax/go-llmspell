// ABOUTME: Tests for script execution logic, covering parameter passing and lifecycle management.
// ABOUTME: Ensures proper execution flow, error handling, and resource cleanup.

package runner

import (
	"context"
	"errors"
	"os"
	"path/filepath"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/lexlapax/go-llmspell/pkg/engine"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

func TestScriptExecutor(t *testing.T) {
	t.Run("new_executor", func(t *testing.T) {
		config := DefaultRunnerConfig()
		registry := engine.NewRegistry(engine.RegistryConfig{})
		manager := NewEngineRegistryManager(registry)
		selector := NewEngineSelector(manager)
		
		executor := NewScriptExecutor(config, manager, selector)
		
		assert.NotNil(t, executor)
		assert.Equal(t, config, executor.config)
		assert.Equal(t, manager, executor.engineManager)
		assert.Equal(t, selector, executor.selector)
		assert.NotNil(t, executor.metrics)
		assert.NotNil(t, executor.semaphore)
	})

	t.Run("initialize", func(t *testing.T) {
		executor := createTestExecutor(t)
		ctx := context.Background()
		
		err := executor.Initialize(ctx)
		assert.NoError(t, err)
	})

	t.Run("execute_script", func(t *testing.T) {
		executor := createTestExecutor(t)
		ctx := context.Background()
		
		// Initialize executor
		err := executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Execute a script
		params := map[string]interface{}{"test": true}
		result, err := executor.Execute(ctx, "return 'hello'", params)
		
		assert.NoError(t, err)
		assert.Equal(t, "mock result", result)
		
		// Check metrics
		metrics := executor.GetMetrics()
		assert.Equal(t, int64(1), metrics.ScriptsExecuted)
		assert.Equal(t, int64(1), metrics.SuccessCount)
		assert.Equal(t, int64(0), metrics.ErrorCount)
	})

	t.Run("execute_with_options", func(t *testing.T) {
		executor := createTestExecutor(t)
		ctx := context.Background()
		
		err := executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Track progress updates
		var progressUpdates []Progress
		progressMutex := &sync.Mutex{}
		
		options := &RunnerOptions{
			Engine: "lua",
			Parameters: map[string]interface{}{
				"option": "value",
			},
			ProgressHandler: func(p Progress) {
				progressMutex.Lock()
				progressUpdates = append(progressUpdates, p)
				progressMutex.Unlock()
			},
		}
		
		result, err := executor.ExecuteWithOptions(ctx, "test script", options)
		assert.NoError(t, err)
		assert.NotNil(t, result)
		
		// Should have received progress updates
		progressMutex.Lock()
		assert.NotEmpty(t, progressUpdates)
		progressMutex.Unlock()
	})

	t.Run("execute_file", func(t *testing.T) {
		executor := createTestExecutor(t)
		ctx := context.Background()
		
		err := executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Create a test file
		tmpDir := t.TempDir()
		scriptFile := filepath.Join(tmpDir, "test.lua")
		err = os.WriteFile(scriptFile, []byte("return 42"), 0644)
		require.NoError(t, err)
		
		// Execute the file
		params := map[string]interface{}{"file": true}
		result, err := executor.ExecuteFile(ctx, scriptFile, params)
		
		assert.NoError(t, err)
		assert.Equal(t, "mock file result", result)
	})

	t.Run("execute_spell", func(t *testing.T) {
		executor := createTestExecutor(t)
		ctx := context.Background()
		
		err := executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Create a spell
		spell := &SpellMetadata{
			Name:       "test-spell",
			Version:    "1.0.0",
			Engine:     "lua",
			EntryPoint: "main.lua",
			Parameters: []SpellParameter{
				{Name: "message", Type: "string", Default: "hello"},
			},
		}
		
		// Execute the spell
		params := map[string]interface{}{"custom": "param"}
		result, err := executor.ExecuteSpell(ctx, spell, params)
		
		assert.NoError(t, err)
		assert.NotNil(t, result)
		assert.Equal(t, "lua", result.Engine)
	})

	t.Run("validate_script", func(t *testing.T) {
		executor := createTestExecutor(t)
		
		// Validation not implemented in mock, but test the interface
		err := executor.Validate("test script")
		assert.NoError(t, err)
	})

	t.Run("concurrent_execution", func(t *testing.T) {
		config := DefaultRunnerConfig()
		config.MaxConcurrentScripts = 2 // Limit concurrency
		
		registry := engine.NewRegistry(engine.RegistryConfig{})
		manager := NewEngineRegistryManager(registry)
		selector := NewEngineSelector(manager)
		executor := NewScriptExecutor(config, manager, selector)
		
		// Register engine
		factory := &mockEngineFactory{name: "lua"}
		err := registry.Register(factory)
		require.NoError(t, err)
		
		ctx := context.Background()
		err = executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Track concurrent executions
		var concurrent int32
		var maxConcurrent int32
		
		// Execute multiple scripts concurrently
		var wg sync.WaitGroup
		for i := 0; i < 5; i++ {
			wg.Add(1)
			go func() {
				defer wg.Done()
				
				// Increment concurrent count
				current := atomic.AddInt32(&concurrent, 1)
				
				// Update max if needed
				for {
					max := atomic.LoadInt32(&maxConcurrent)
					if current <= max || atomic.CompareAndSwapInt32(&maxConcurrent, max, current) {
						break
					}
				}
				
				// Simulate some work
				time.Sleep(50 * time.Millisecond)
				
				options := &RunnerOptions{Engine: "lua"}
				_, _ = executor.ExecuteWithOptions(ctx, "concurrent test", options)
				
				// Decrement concurrent count
				atomic.AddInt32(&concurrent, -1)
			}()
		}
		
		wg.Wait()
		
		// Should not exceed max concurrent scripts
		assert.LessOrEqual(t, maxConcurrent, int32(2))
	})

	t.Run("timeout_handling", func(t *testing.T) {
		executor := createTestExecutor(t)
		
		// Use a very short timeout
		ctx, cancel := context.WithTimeout(context.Background(), 1*time.Millisecond)
		defer cancel()
		
		err := executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Sleep to ensure timeout
		time.Sleep(5 * time.Millisecond)
		
		// Try to execute - should respect context cancellation
		_, err = executor.Execute(ctx, "test", nil)
		// Mock doesn't check context, but real implementation should
		// In real implementation, this would return context.DeadlineExceeded
	})

	t.Run("error_handling", func(t *testing.T) {
		executor := createTestExecutor(t)
		ctx := context.Background()
		
		err := executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Register an engine that returns errors
		registry := executor.engineManager.registry
		errorFactory := &mockEngineFactory{
			name:        "error-engine",
			createError: errors.New("engine creation failed"),
		}
		err = registry.Register(errorFactory)
		require.NoError(t, err)
		
		// Try to execute with error engine
		options := &RunnerOptions{Engine: "error-engine"}
		result, err := executor.ExecuteWithOptions(ctx, "test", options)
		
		assert.Error(t, err)
		assert.Nil(t, result)
		
		// Check error metrics
		metrics := executor.GetMetrics()
		assert.Equal(t, int64(1), metrics.ErrorCount)
	})

	t.Run("shutdown", func(t *testing.T) {
		executor := createTestExecutor(t)
		ctx := context.Background()
		
		err := executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Execute some scripts
		_, _ = executor.Execute(ctx, "test1", nil)
		_, _ = executor.Execute(ctx, "test2", nil)
		
		// Shutdown
		err = executor.Shutdown()
		assert.NoError(t, err)
		
		// Metrics should still be available after shutdown
		metrics := executor.GetMetrics()
		assert.Equal(t, int64(2), metrics.ScriptsExecuted)
	})
}

func TestExecutionResult_Helpers(t *testing.T) {
	t.Run("successful_result", func(t *testing.T) {
		result := &ExecutionResult{
			Value:     "success",
			Duration:  100 * time.Millisecond,
			Engine:    "lua",
			StartTime: time.Now().Add(-100 * time.Millisecond),
			EndTime:   time.Now(),
		}
		
		assert.True(t, result.IsSuccess())
		assert.False(t, result.IsError())
		assert.Nil(t, result.Error)
	})

	t.Run("error_result", func(t *testing.T) {
		result := &ExecutionResult{
			Error:     errors.New("execution failed"),
			Duration:  50 * time.Millisecond,
			Engine:    "javascript",
			StartTime: time.Now().Add(-50 * time.Millisecond),
			EndTime:   time.Now(),
		}
		
		assert.False(t, result.IsSuccess())
		assert.True(t, result.IsError())
		assert.NotNil(t, result.Error)
	})
}

func TestSignalHandling(t *testing.T) {
	t.Run("graceful_shutdown_on_cancel", func(t *testing.T) {
		executor := createTestExecutor(t)
		ctx, cancel := context.WithCancel(context.Background())
		
		err := executor.Initialize(ctx)
		require.NoError(t, err)
		
		// Start a "long-running" script in background
		done := make(chan bool)
		go func() {
			_, _ = executor.Execute(ctx, "long script", nil)
			done <- true
		}()
		
		// Cancel context
		cancel()
		
		// Wait for completion
		select {
		case <-done:
			// Good, execution completed
		case <-time.After(1 * time.Second):
			t.Fatal("execution did not complete after context cancellation")
		}
	})
}

// Helper function to create a test executor
func createTestExecutor(t *testing.T) *ScriptExecutor {
	config := DefaultRunnerConfig()
	registry := engine.NewRegistry(engine.RegistryConfig{})
	
	// Register a mock engine
	factory := &mockEngineFactory{
		name:           "lua",
		fileExtensions: []string{"lua"},
	}
	err := registry.Register(factory)
	require.NoError(t, err)
	
	manager := NewEngineRegistryManager(registry)
	selector := NewEngineSelector(manager)
	
	return NewScriptExecutor(config, manager, selector)
}

// Benchmark tests
func BenchmarkScriptExecutor_Execute(b *testing.B) {
	executor := createTestExecutor(&testing.T{})
	ctx := context.Background()
	_ = executor.Initialize(ctx)
	
	params := map[string]interface{}{"benchmark": true}
	
	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		_, _ = executor.Execute(ctx, "benchmark script", params)
	}
}

func BenchmarkScriptExecutor_Concurrent(b *testing.B) {
	executor := createTestExecutor(&testing.T{})
	ctx := context.Background()
	_ = executor.Initialize(ctx)
	
	b.RunParallel(func(pb *testing.PB) {
		for pb.Next() {
			_, _ = executor.Execute(ctx, "concurrent benchmark", nil)
		}
	})
}